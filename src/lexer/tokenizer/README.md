# üß© Tokenizer ‚Äî Vis√£o Geral

O m√≥dulo **tokenizer** transforma a linha bruta do usu√°rio em uma sequ√™ncia estruturada de **tokens**, cada um contendo seu valor, tipo, coordenadas e callbacks de expans√£o.
√â o elo entre o *splitter* (que apenas divide a linha) e o *expander* (que interpreta `$VAR`, globs e aspas).

O objetivo do tokenizer √©:
1. **Criar tokens bem definidos** a partir dos chunks gerados pelo splitter.
2. **Classificar cada token** (pipe, redirecionamento, operador, palavra comum).
3. **Conectar cada token ao sistema de expans√£o**, anexando callbacks e estruturas auxiliares.
4. **Fornecer um objeto final (`t_tokenized_prompt`)**, pronto para a fase de *expans√£o* e, depois, *execu√ß√£o*.

---

# üîß Funcionamento Geral

O fluxo b√°sico √©:

1. O usu√°rio digita algo como:
	```bash
	echo "$USER" | grep foo > out.txt
	```

2. O `splitter` devolve chunks j√° segmentados por espa√ßos e aspas.

3. O `tokenizer` recebe esses chunks e, para cada um:
- Determina o tipo do token (`|`, `>`, `>>`, palavra comum etc.).
- Atribui callbacks para expans√µes.
- Salva a substring bruta.
- Armazena as coordenadas originais na linha.
- Monta um objeto `t_token`.

4. Todos os tokens s√£o guardados em um `t_tokenized_prompt`, que cont√©m:
- o vetor de tokens,
- a linha original,
- o tamanho total,
- um destrutor pr√≥prio.

Ap√≥s isso, o *expander* pode operar no token com seguran√ßa, usando as coordenadas originais e os callbacks.

---

# üß± Estruturas Principais

## **t_tokenized_prompt**

Cont√©m o resultado completo da tokeniza√ß√£o.

- `tokens`: vetor de ponteiros para `t_token`.
- `original_prompt`: c√≥pia exata da linha digitada.
- `size`: n√∫mero total de tokens.
- `destroy`: fun√ß√£o para limpar tudo de forma segura.

Serve como **container principal**, preservando o estado antes da expans√£o.

---

# üî® Fun√ß√µes Essenciais

## **ft_tokenizer(prompt, expand_var, expand_glob)**

Fun√ß√£o de entrada.
Une o splitter e o tokenizer:

- Chama `ft_splitter()` ‚Üí obt√©m os chunks.
- Prepara callbacks de expans√£o.
- Cria a estrutura final via `ft_create_tokenized_prompt()`.
- Retorna um `t_tokenized_prompt`.

√â o ponto de partida da fase de tokeniza√ß√£o.

---

## **ft_create_tokenized_prompt(prompt, &splited, callbacks)**

Constr√≥i o objeto final:

- Aloca `t_tokenized_prompt`.
- Copia a prompt original.
- Cria o vetor de tokens com tamanho exato.
- Para cada chunk do splitter:
- chama `ft_tokenize()` para construir o token final.
- Em caso de erro, destr√≥i tudo com seguran√ßa.
- Registra o destrutor.

√â o cora√ß√£o da constru√ß√£o do tokenizer.

---

## **ft_destroy_tokenized_prompt(self_ref)**

Destrutor completo:

- Libera a string original.
- Libera cada token individual.
- Libera o vetor de tokens.
- Libera a pr√≥pria estrutura.
- Zera o ponteiro externo.

Evita leak em qualquer contexto.

---

## **ft_coord_dup() / ft_new_coord()**

Fun√ß√µes utilit√°rias simples:

- `ft_coord_dup` ‚Üí duplica um par `[start, end]`.
- `ft_new_coord` ‚Üí cria um par `[start, end]` novo.

Essenciais para manter coordenadas independentes ao longo do fluxo.

---

# üìå Resumo Final

O tokenizer:

- Recebe chunks brutos do splitter.
- Cria tokens completos com tipo, posi√ß√£o e callbacks.
- Embala tudo em um `t_tokenized_prompt`.
- Prepara a linha para as fases de expans√£o (vari√°veis, globs, aspas) e execu√ß√£o.

√â o **n√∫cleo da interpreta√ß√£o lexical** do shell.

